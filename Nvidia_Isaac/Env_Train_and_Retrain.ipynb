{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f4d4e3",
   "metadata": {},
   "source": [
    "## !export VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/nvidia_icd.json\n",
    "!export LD_LIBRARY_PATH=/home/t1tl/miniconda3/envs/rlgpu/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de26567",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06a4c85",
   "metadata": {
    "code_folding": [
     168
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_37' (/home/t1tl/Downloads/IsaacGym_Preview_2_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/t1tl/Downloads/IsaacGym_Preview_2_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 1.8.1\n",
      "Device count 1\n",
      "/home/t1tl/Downloads/IsaacGym_Preview_2_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch\n",
      "Using /home/t1tl/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /home/t1tl/.cache/torch_extensions/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module gymtorch...\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from isaacgym import gymapi\n",
    "from isaacgym import gymtorch\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "class BasicEnv(gym.Env):\n",
    "    def __init__(self, normalize):\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "#         self.observation_space = gym.spaces.Box(low=np.array([0,-1.2]),high=np.array([1.2,0]),shape=(2,))\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0,1]),high=np.array([0,1]),shape=(2,))\n",
    "\n",
    "        self.obs_spc = [3,3]\n",
    "        gymI = gymapi.acquire_gym()\n",
    "\n",
    "        sim_params = gymapi.SimParams()\n",
    "\n",
    "        # set common parameters\n",
    "        sim_params.dt = 1 / 60\n",
    "        sim_params.substeps = 2\n",
    "        sim_params.up_axis = gymapi.UP_AXIS_Z\n",
    "        sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.8)\n",
    "\n",
    "        # set PhysX-specific parameters\n",
    "        sim_params.physx.use_gpu = True\n",
    "        sim_params.physx.solver_type = 1\n",
    "        sim_params.physx.num_position_iterations = 3\n",
    "        sim_params.physx.num_velocity_iterations = 1\n",
    "        sim_params.physx.contact_offset = 0.01\n",
    "        sim_params.physx.rest_offset = 0.0\n",
    "\n",
    "        # set Flex-specific parameters\n",
    "        sim_params.flex.solver_type = 5\n",
    "        sim_params.flex.num_outer_iterations = 4\n",
    "        sim_params.flex.num_inner_iterations = 20\n",
    "        sim_params.flex.relaxation = 0.8\n",
    "        sim_params.flex.warm_start = 0.5\n",
    "        sim_params.up_axis = gymapi.UP_AXIS_Z\n",
    "        sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.8)\n",
    "\n",
    "        # create sim with these parameters\n",
    "        sim = gymI.create_sim(0, 0, gymapi.SIM_PHYSX, sim_params)\n",
    "\n",
    "        plane_params = gymapi.PlaneParams()\n",
    "        plane_params.normal = gymapi.Vec3(0, 0, 1) # z-up!\n",
    "        plane_params.distance = 0\n",
    "        plane_params.static_friction = 1\n",
    "        plane_params.dynamic_friction = 1\n",
    "        plane_params.restitution = 0\n",
    "\n",
    "        # create the ground plane\n",
    "        gymI.add_ground(sim, plane_params)\n",
    "#         num_envs = 9\n",
    "        envs_per_row = 8\n",
    "        env_spacing = 4.0\n",
    "        env_lower = gymapi.Vec3(-env_spacing, 0.0, -env_spacing)\n",
    "        env_upper = gymapi.Vec3(env_spacing, env_spacing, env_spacing)\n",
    "\n",
    "        self.vel_app = 1\n",
    "        self.effort_app=30\n",
    "        # cache some common handles for later use\n",
    "\n",
    "        env = gymI.create_env(sim, env_lower, env_upper, envs_per_row)\n",
    "        \n",
    "        #ADD CUSTOM PATH\n",
    "        asset_root = \"/home/t1tl/Documents/UniGalway/courses/thesis/\"\n",
    "        asset_file = \"URDFs/krawler3.urdf\"\n",
    "        asset = gymI.load_asset(sim, asset_root, asset_file)\n",
    "\n",
    "#         height = random.uniform(1.0, 2.5)\n",
    "\n",
    "        pose = gymapi.Transform()\n",
    "        pose.p = gymapi.Vec3(0.0, 0, 0.6)\n",
    "\n",
    "        actor_handle = gymI.create_actor(env, asset, pose, \"MyActor\", 0, 1)\n",
    "        rh = gymI.get_actor_rigid_shape_properties(env,actor_handle)\n",
    "        rh[0].friction=0\n",
    "        rh[1].friction=0\n",
    "        rh[-1].friction=10\n",
    "        gymI.set_actor_rigid_shape_properties(env,actor_handle,rh)\n",
    "#         actor_handles.append(actor_handle)\n",
    "        cam_props = gymapi.CameraProperties()\n",
    "        self.viewer = gymI.create_viewer(sim, cam_props)\n",
    "        props = gymI.get_actor_dof_properties(env, actor_handle)\n",
    "        props[\"driveMode\"].fill(gymapi.DOF_MODE_POS)\n",
    "#         props[\"stiffness\"].fill(100000.0)\n",
    "        props[\"damping\"].fill(200.0)\n",
    "        props[\"effort\"].fill(2000.0)\n",
    "        props[\"stiffness\"].fill(3000.0)\n",
    "        props[\"friction\"].fill(100.0)\n",
    "        props[\"damping\"].fill(0.0)\n",
    "#         props[\"effort\"].fill(20000.0)\n",
    "        gymI.set_actor_dof_properties(env, actor_handle, props)\n",
    "        \n",
    "        self.gymI = gymI\n",
    "        self.env = env\n",
    "        self.actor_handle = actor_handle\n",
    "        self.sim=sim\n",
    "        self.props = gymI.get_actor_dof_properties(env,actor_handle)\n",
    "        \n",
    "        self.targets = np.array([self.val_to_bin(0,0),self.val_to_bin(0,1)])\n",
    "        self.state = self.get_state()\n",
    "        t = gymI.get_rigid_transform(env,0)\n",
    "        self.last_pos = t.p.y\n",
    "        self.step_count = 0\n",
    "        self.tmr = time.time()\n",
    "        self.draw = False\n",
    "        self.eprew = 0\n",
    "        \n",
    "        body_states = self.gymI.get_env_rigid_body_states(self.env, gymapi.STATE_ALL)\n",
    "        self.rot = body_states[\"pose\"][\"r\"].copy()\n",
    "        self.pos = body_states[\"pose\"][\"p\"].copy()\n",
    "        self.bs = body_states.copy()\n",
    "        self.verbose=0\n",
    "        \n",
    "    def step(self, action):\n",
    "        info = {}\n",
    "        done=False\n",
    "        self.old_state=self.get_state()\n",
    "        self.targets=self.state\n",
    "        self.apply_action(action)\n",
    "#         self.gymI.set_actor_dof_position_targets(self.env, self.actor_handle, [self.bin_to_val(0),self.bin_to_val(1)])\n",
    "#         print(\"state:\",a, )\n",
    "        \n",
    "        self.simulate()\n",
    "        t = self.gymI.get_rigid_transform(self.env,0)\n",
    "        curr_pos = t.p.y\n",
    "        reward = (self.last_pos - curr_pos)*-1\n",
    "        \n",
    "#         print('r',reward,self.last_pos,curr_pos)\n",
    "        if abs(reward)<0.02:\n",
    "            reward=0\n",
    "        else:\n",
    "            reward*=10\n",
    "        self.step_count+=1\n",
    "        if self.step_count>100:\n",
    "            done=True\n",
    "        self.last_pos=curr_pos\n",
    "        if self.last_pos >2:\n",
    "            print(\"done\")\n",
    "            reward +=30\n",
    "            done=True\n",
    "        reward -=0.1\n",
    "        self.state=self.get_state()\n",
    "        s = self.state\n",
    "        self.eprew+=reward\n",
    "        if reward!=-0.1 and self.verbose>0:\n",
    "            print(round(reward,2),\"|\",end=\"\")\n",
    "        return s, reward , done , info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.gymI.set_actor_dof_states(self.env,self.actor_handle,[0,0],gymapi.STATE_POS)\n",
    "        self.gymI.set_actor_dof_position_targets(self.env, self.actor_handle, [0,0])\n",
    "        self.targets=[0,1]\n",
    "\n",
    "        self.step_count=0\n",
    "        print(\"|%.2f|%.4f\"%(time.time()-self.tmr,self.eprew))\n",
    "        self.tmr=time.time()\n",
    "        self.eprew=0\n",
    "        self.gymI.set_env_rigid_body_states(self.env, self.bs, gymapi.STATE_ALL)\n",
    "        t = self.gymI.get_rigid_transform(self.env,0)\n",
    "        curr_pos = t.p.y\n",
    "        self.last_pos=curr_pos\n",
    "\n",
    "        return self.get_state()\n",
    "    \n",
    "    def close (self):\n",
    "        self.gymI.destroy_viewer(self.viewer)\n",
    "        self.gymI.destroy_sim(self.sim)\n",
    "        \n",
    "    def get_state(self):\n",
    "        s0 = self.gymI.get_dof_position(self.env,0)\n",
    "        s1 = self.gymI.get_dof_position(self.env,1)\n",
    "        ds0 = self.val_to_bin(s0,0)\n",
    "        ds1 = self.val_to_bin(s1,1)\n",
    "        sl = [s0,s1]\n",
    "        return np.round( \\\n",
    "                    np.clip( \\\n",
    "                     np.array( [(sl[i]-self.props[i][1])/(self.props[i][2]-self.props[i][1]) \\\n",
    "                              for i in range(2)]),0,1),decimals=2)\n",
    "    \n",
    "    def apply_action(self,a):\n",
    "        add_to = np.array([0,0])\n",
    "        if a ==0:\n",
    "            add_to = np.array([1,0])\n",
    "        if a ==1:\n",
    "            add_to = np.array([0,1])\n",
    "        if a ==2:\n",
    "            add_to = np.array([-1,0])\n",
    "        if a==3:\n",
    "            add_to = np.array([0,-1])\n",
    "        targets = np.clip(self.get_state() - 0.2*add_to,0,1)\n",
    "        self.gymI.set_actor_dof_position_targets(self.env,self.actor_handle, self.toTargets(targets))\n",
    "\n",
    "    \n",
    "    def toTargets(self, targets):\n",
    "        targs = []\n",
    "        for s in range(2):\n",
    "            vmin=self.props[s][1]\n",
    "            vmax=self.props[s][2]\n",
    "            targs.append(self.translate(targets[s],0,1,vmin,vmax))\n",
    "        return targs\n",
    "\n",
    "            \n",
    "    def translate(self,value, leftMin, leftMax, rightMin, rightMax):\n",
    "        # Figure out how 'wide' each range is\n",
    "        leftSpan = leftMax - leftMin\n",
    "        rightSpan = rightMax - rightMin\n",
    "\n",
    "        # Convert the left range into a 0-1 range (float)\n",
    "        valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "        # Convert the 0-1 range into a value in the right range.\n",
    "        return rightMin + (valueScaled * rightSpan)\n",
    "        \n",
    "    def bin_to_val(self,s):\n",
    "        vbin = self.targets[s]\n",
    "        vmin=self.props[s][1]\n",
    "        vmax=self.props[s][2]\n",
    "        return (vbin/self.obs_spc[s])*(vmax-vmin)+vmin\n",
    "    \n",
    "    def val_to_bin(self,val,s):\n",
    "        vmin=self.props[s][1]\n",
    "        vmax=self.props[s][2]\n",
    "        return round((max((val-vmin),0)/(vmax-vmin))*self.obs_spc[s])\n",
    "        \n",
    "    def render(self):\n",
    "        self.draw=True\n",
    "    def simulate(self):\n",
    "        sims = random.randint(30,40)#add jitter to exncourage generalization\n",
    "        for i in range(sims):\n",
    "\n",
    "            # step the physics\n",
    "            self.gymI.simulate(self.sim)\n",
    "            self.gymI.fetch_results(self.sim, True)\n",
    "\n",
    "            # update the viewer\n",
    "            if self.draw:\n",
    "                self.gymI.step_graphics(self.sim);\n",
    "                self.gymI.draw_viewer(self.viewer, self.sim, True)\n",
    "\n",
    "            # Wait for dt to elapse in real time.\n",
    "            # This synchronizes the physics simulation with the rendering rate.\n",
    "#             self.gymI.sync_frame_time(self.sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7effddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d479c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t1tl/miniconda3/envs/rlgpu2/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "if 'env' in locals():\n",
    "    env.close()\n",
    "env = BasicEnv(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34906e",
   "metadata": {},
   "source": [
    "# DEEP Q LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75cb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a0619",
   "metadata": {},
   "source": [
    "## train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b49f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel(model,env,draw=False):\n",
    "    print(\"Evaluating...\")\n",
    "    env.draw = draw\n",
    "    obs = env.reset()\n",
    "    done=False\n",
    "    accRew = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(action,\"->\",obs,round(reward,2))\n",
    "        accRew+=reward\n",
    "    print(\"Evaluated\",accRew)\n",
    "    env.draw=False\n",
    "    return accRew\n",
    "#         if done:\n",
    "#             obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ce3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQNs\n",
    "\n",
    "\n",
    "#gym.make(\"CartPole-v1\")\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# model.learn(total_timesteps=10000)\n",
    "def trainDQN(time_steps,learning_rate, h1,h2,batch_size,gradient_steps,iteration,env):\n",
    "    env.draw=False\n",
    "#     policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "#                          net_arch=[h1,h2])\n",
    "    policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                             net_arch=[h1])\n",
    "    idstr = \"DQN_LR%.3f_H1_%d_H2_%d_BS_%d_GS_%d_%d\"%(learning_rate,h1,h2,batch_size,gradient_steps,iteration)\n",
    "    model=DQN(\"MlpPolicy\", env, verbose=2,batch_size=batch_size,\\\n",
    "              learning_starts=3000,learning_rate=learning_rate,\\\n",
    "              gradient_steps=gradient_steps,train_freq=(2,\"episode\"),\\\n",
    "              target_update_interval=4000,\\\n",
    "              exploration_fraction=0.4, tensorboard_log=\"tlog/dqn_evals_6\",\\\n",
    "              policy_kwargs=policy_kwargs)\n",
    "    model.learn(total_timesteps=time_steps, log_interval=40,eval_env=env, eval_freq=5000,\\\n",
    "                n_eval_episodes=3,eval_log_path=\"./logs/\",tb_log_name=idstr)\n",
    "    model.save(\"models/\"+idstr)\n",
    "    ev = evalModel(model,env)\n",
    "    return ev\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1333843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.01,64,32,1024]\n",
    "# vals = [[0.01,32,16,1024],[0.01,64,32,512],[0.001,64,32,1024],[0.1,64,32,1024],[0.001,32,16,512]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f019dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = [[0.01,32,16,1024],[0.01,32,16,4048],[0.001,32,16,4048]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e125b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = [[0.01,64,64,5000,10],[0.01,32,16,5000,10]]#, [0.01,32,16,2048]]#\n",
    "#[0.01,32,32,5000,10]]\n",
    "vals = [[0.01,64,0,5000,10]]#[[0.01,32,32,5000,10]]#[0.01,64,64,1000,10],[0.001,64,64,5000,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02d7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb297bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [0.01, 64, 0, 5000, 10] 0\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "|111.22|0.0000\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to tlog/dqn_evals_6/DQN_LR0.010_H1_64_H2_0_BS_5000_GS_10_0_1\n",
      "|5.84|-7.8814\n",
      "|4.53|-10.2239\n",
      "|4.38|-4.3075\n",
      "|4.45|-10.2595\n",
      "|4.48|-13.7189\n",
      "|4.54|-7.9418\n",
      "|5.16|-8.2057\n",
      "|4.81|-7.3976\n",
      "|4.85|-4.0126\n",
      "|4.72|-14.1929\n",
      "|4.44|-20.6889\n",
      "|4.42|-9.3343\n",
      "|4.46|-8.5007\n",
      "|4.46|-8.2047\n",
      "|4.49|-7.0604\n",
      "|4.39|-10.4954\n",
      "|4.39|-7.9410\n",
      "|4.91|-10.4126\n",
      "|5.09|-8.9296\n",
      "|4.57|-10.9259\n",
      "|4.64|-7.7604\n",
      "|4.97|-6.6801\n",
      "|4.62|-11.7162\n",
      "|4.49|-7.6742\n",
      "|4.57|-9.6554\n",
      "|4.61|-14.8467\n",
      "|4.86|-3.6698\n",
      "|4.57|-9.3914\n",
      "|4.65|-11.2865\n",
      "|5.86|-7.6178\n",
      "|4.62|-13.7338\n",
      "|4.54|-7.6925\n",
      "|4.56|-8.9245\n",
      "|4.47|-7.5896\n",
      "|4.53|-6.4923\n",
      "|4.51|-11.7995\n",
      "|4.50|-7.4027\n",
      "|4.67|-0.3374\n",
      "|4.80|-4.9914\n",
      "|4.75|-13.4712\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | -9.08    |\n",
      "|    exploration rate | 0.852    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total timesteps  | 4040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0333   |\n",
      "|    n_updates        | 50       |\n",
      "----------------------------------\n",
      "|4.64|-11.3729\n",
      "|4.66|-15.4510\n",
      "|4.69|-2.0869\n",
      "|4.57|-5.2600\n",
      "|4.61|-5.7623\n",
      "|4.52|-16.7652\n",
      "|4.52|-8.5312\n",
      "|4.47|-10.8793\n",
      "|4.65|-8.6326\n",
      "|2.31|-1.0630\n",
      "|4.63|-0.2094\n",
      "|4.65|-0.2096\n",
      "|4.58|-0.1594\n",
      "Eval num_timesteps=5000, episode_reward=-0.19 +/- 0.02\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.193   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0414   |\n",
      "|    n_updates        | 100      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|4.66|-12.6248\n",
      "|4.73|-9.3802\n",
      "|4.81|-19.3980\n",
      "|4.64|-12.9513\n",
      "|5.02|-11.5165\n",
      "|4.84|-19.9076\n",
      "|4.65|-4.7039\n",
      "|4.62|-13.1259\n",
      "|4.55|-9.3039\n",
      "|4.60|-8.4868\n",
      "|4.48|-5.2892\n",
      "|4.53|-6.3787\n",
      "|4.53|-8.5457\n",
      "|4.57|-3.1324\n",
      "|4.54|-15.2900\n",
      "|4.59|-14.0881\n",
      "|4.57|-10.7708\n",
      "|4.53|0.7387\n",
      "|4.54|-8.6095\n",
      "|4.61|-13.3215\n",
      "|4.55|-9.1052\n",
      "|4.53|-2.2785\n",
      "|4.55|-14.1540\n",
      "|4.58|-7.7342\n",
      "|4.81|-9.4479\n",
      "|4.62|-5.7358\n",
      "|4.50|-9.9140\n",
      "|4.61|-11.0514\n",
      "|4.53|-4.3574\n",
      "|4.55|-6.3033\n",
      "|4.53|-12.6706\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -9.35    |\n",
      "|    exploration rate | 0.703    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 387      |\n",
      "|    total timesteps  | 8131     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0355   |\n",
      "|    n_updates        | 250      |\n",
      "----------------------------------\n",
      "|4.59|-8.3830\n",
      "|4.59|-8.4719\n",
      "|4.58|-3.2494\n",
      "|4.56|-10.3635\n",
      "|4.57|-7.7856\n",
      "|4.57|-8.5965\n",
      "|4.63|-14.6125\n",
      "|4.49|-9.5418\n",
      "|4.47|-12.6898\n",
      "|4.52|-1.4369\n",
      "|4.60|-7.2717\n",
      "|4.58|-7.3440\n",
      "|4.55|-4.5681\n",
      "|4.51|-6.2241\n",
      "|4.63|-13.7127\n",
      "|4.61|-5.2385\n",
      "|4.52|-10.5735\n",
      "|4.50|-5.7698\n",
      "|2.32|-2.5993\n",
      "|4.57|-0.5481\n",
      "|4.56|-0.2104\n",
      "|4.63|-0.2099\n",
      "Eval num_timesteps=10000, episode_reward=-0.32 +/- 0.16\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.323   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.635    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0435   |\n",
      "|    n_updates        | 350      |\n",
      "----------------------------------\n",
      "|4.67|-18.0084\n",
      "|4.63|-9.8251\n",
      "|4.71|-11.4492\n",
      "|4.71|-6.8987\n",
      "|4.67|-7.6206\n",
      "|4.74|-9.5587\n",
      "|4.64|-10.0518\n",
      "|4.88|-2.2420\n",
      "|4.71|-8.3110\n",
      "|4.70|-9.2641\n",
      "|4.68|-6.2873\n",
      "|5.07|-6.0022\n",
      "|5.05|-13.3014\n",
      "|5.04|-7.8168\n",
      "|4.66|-3.8245\n",
      "|4.68|-3.5321\n",
      "|4.71|-4.4348\n",
      "|4.84|1.3473\n",
      "|4.60|-3.1079\n",
      "|4.68|-5.5948\n",
      "|4.57|-11.4812\n",
      "|4.57|-12.9536\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -8.76    |\n",
      "|    exploration rate | 0.553    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 590      |\n",
      "|    total timesteps  | 12222    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 450      |\n",
      "----------------------------------\n",
      "|4.67|-9.4842\n",
      "|4.78|-7.2906\n",
      "|4.75|-12.3777\n",
      "|4.88|-3.8180\n",
      "|4.68|-5.8701\n",
      "|4.81|-2.6532\n",
      "|4.75|-2.9060\n",
      "|4.73|-4.6681\n",
      "|4.81|-6.5112\n",
      "|4.67|-11.2767\n",
      "|4.88|-2.7778\n",
      "|4.65|-1.7466\n",
      "|4.69|-5.2522\n",
      "|4.62|-6.0351\n",
      "|4.78|-6.3640\n",
      "|4.82|-8.0168\n",
      "|5.04|-7.5997\n",
      "|5.07|-7.7700\n",
      "|5.07|-5.0366\n",
      "|4.77|-5.7592\n",
      "|5.11|-8.7861\n",
      "|4.63|-3.0568\n",
      "|4.79|-6.6362\n",
      "|5.16|-7.5738\n",
      "|4.88|-4.1974\n",
      "|4.62|-4.2137\n",
      "|4.47|-3.8814\n",
      "|2.27|4.4238\n",
      "|4.46|-0.1581\n",
      "|4.45|-0.2075\n",
      "|4.85|-0.2076\n",
      "Eval num_timesteps=15000, episode_reward=-0.19 +/- 0.02\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.191   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0499   |\n",
      "|    n_updates        | 590      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|5.53|-3.5030\n",
      "|4.81|-2.1127\n",
      "|4.82|-3.6479\n",
      "|5.82|-5.9416\n",
      "|4.89|-6.2989\n",
      "|5.34|-7.8656\n",
      "|5.51|-7.0362\n",
      "|5.10|-6.0233\n",
      "|4.80|-3.1925\n",
      "|4.79|-5.5728\n",
      "|4.67|-4.5629\n",
      "|4.58|-3.3522\n",
      "|4.56|-6.0353\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -7.13    |\n",
      "|    exploration rate | 0.404    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 800      |\n",
      "|    total timesteps  | 16313    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 650      |\n",
      "----------------------------------\n",
      "|4.74|-13.4429\n",
      "|4.96|-8.2582\n",
      "|5.39|-4.1910\n",
      "|5.32|-12.1192\n",
      "|4.83|-2.0319\n",
      "|4.61|-5.2021\n",
      "|4.58|-4.1920\n",
      "|4.57|1.0633\n",
      "|4.75|-7.3910\n",
      "|4.61|-5.3487\n",
      "|4.49|-5.4442\n",
      "|4.58|-4.3409\n",
      "|4.75|-8.6727\n",
      "|4.73|-4.9461\n",
      "|4.64|-6.9824\n",
      "|4.61|-13.8466\n",
      "|4.62|-5.5555\n",
      "|4.67|-4.0057\n",
      "|4.55|-4.3355\n",
      "|4.85|-4.1746\n",
      "|5.01|-0.2976\n",
      "|5.02|-3.8670\n",
      "|5.11|-5.0154\n",
      "|4.81|-5.0566\n",
      "|4.78|-7.7553\n",
      "|4.81|-2.7223\n",
      "|4.68|-6.6869\n",
      "|4.56|-1.3013\n",
      "|4.66|-3.0563\n",
      "|4.94|-5.0747\n",
      "|4.67|-2.7928\n",
      "|4.70|-4.9366\n",
      "|4.78|-3.8059\n",
      "|4.55|-2.8462\n",
      "|4.73|-6.7267\n",
      "|4.70|-3.9370\n",
      "|2.36|3.7729\n",
      "|4.67|-0.1585\n",
      "|4.55|-0.2076\n",
      "|4.63|-0.3275\n",
      "Eval num_timesteps=20000, episode_reward=-0.23 +/- 0.07\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.231   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.269    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0485   |\n",
      "|    n_updates        | 840      |\n",
      "----------------------------------\n",
      "|4.66|-2.1023\n",
      "|4.68|-5.1624\n",
      "|4.55|-7.7638\n",
      "|4.55|-7.4733\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -5.73    |\n",
      "|    exploration rate | 0.254    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1006     |\n",
      "|    total timesteps  | 20404    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.152    |\n",
      "|    n_updates        | 850      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|4.64|-3.6703\n",
      "|4.65|-2.0810\n",
      "|4.65|-7.6351\n",
      "|4.66|-1.4175\n",
      "|4.89|-1.1445\n",
      "|4.67|-3.3963\n",
      "|4.54|-2.0535\n",
      "|4.58|-1.9164\n",
      "|4.59|-6.0221\n",
      "|4.63|-1.4433\n",
      "|4.56|-4.5776\n",
      "|4.58|-6.2142\n",
      "|4.56|-1.7373\n",
      "|4.67|-1.7478\n",
      "|4.69|-0.6272\n",
      "|4.62|-3.1629\n",
      "|4.69|-1.6738\n",
      "|4.68|-1.7434\n",
      "|4.54|-1.9912\n",
      "|4.62|-3.4996\n",
      "|4.66|-3.3561\n",
      "|4.69|-0.5893\n",
      "|4.77|-3.0408\n",
      "|4.66|-1.6421\n",
      "|4.74|-3.1483\n",
      "|4.86|-2.0719\n",
      "|5.38|-0.9586\n",
      "|5.90|-0.5514\n",
      "|4.68|-3.0373\n",
      "|4.89|-4.5574\n",
      "|5.07|-0.9376\n",
      "|4.79|0.5154\n",
      "|4.81|-0.3409\n",
      "|4.89|-0.3266\n",
      "|4.75|-2.9079\n",
      "|4.69|-0.7575\n",
      "|4.66|-1.2618\n",
      "|4.90|-1.4874\n",
      "|4.73|-1.4988\n",
      "|4.58|-0.7468\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -3.98    |\n",
      "|    exploration rate | 0.107    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1196     |\n",
      "|    total timesteps  | 24444    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0717   |\n",
      "|    n_updates        | 1050     |\n",
      "----------------------------------\n",
      "|5.02|-0.0984\n",
      "|4.85|-1.2759\n",
      "|4.83|-3.3345\n",
      "|4.68|-1.1463\n",
      "|4.66|-0.1450\n",
      "|2.27|3.9295\n",
      "|4.62|-0.2091\n",
      "|4.70|-0.2074\n",
      "|4.68|-0.2082\n",
      "Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.208   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.0866   |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0501   |\n",
      "|    n_updates        | 1080     |\n",
      "----------------------------------\n",
      "|4.59|-0.3906\n",
      "|4.67|-0.2314\n",
      "|4.70|-0.8502\n",
      "|4.68|-0.5815\n",
      "|4.83|-0.4716\n",
      "|4.65|-0.8615\n",
      "|4.69|-1.1412\n",
      "|5.35|-1.1547\n",
      "|4.94|-1.6447\n",
      "|4.72|-2.4612\n",
      "|4.86|-0.5293\n",
      "|4.51|-0.7573\n",
      "|4.57|-0.1897\n",
      "|5.04|-0.2099\n",
      "|4.73|0.5403\n",
      "|4.77|-1.4296\n",
      "|4.75|-0.8664\n",
      "|4.73|-0.2174\n",
      "|4.68|-2.9810\n",
      "|4.88|-0.2087\n",
      "|5.00|-0.2674\n",
      "|4.99|-0.0175\n",
      "|4.88|-0.2093\n",
      "|5.03|-0.7756\n",
      "|4.82|-0.2315\n",
      "|5.49|-1.5857\n",
      "|4.93|-1.1596\n",
      "|4.84|-0.2068\n",
      "|5.26|-0.2431\n",
      "|4.88|-4.0611\n",
      "|4.76|-3.3543\n",
      "|4.77|-1.8377\n",
      "|4.63|-0.1897\n",
      "|4.69|-0.4242\n",
      "|4.57|-0.3462\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -2.09    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1405     |\n",
      "|    total timesteps  | 28535    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.063    |\n",
      "|    n_updates        | 1250     |\n",
      "----------------------------------\n",
      "|4.67|-0.2075\n",
      "|4.67|-0.1296\n",
      "|5.06|-0.7166\n",
      "|4.94|-0.3129\n",
      "|4.98|-0.7794\n",
      "|5.21|-1.0637\n",
      "|4.82|-0.8166\n",
      "|4.53|-0.7049\n",
      "|4.96|-0.4063\n",
      "|4.64|-1.9743\n",
      "|4.65|-1.6491\n",
      "|4.93|-0.5560\n",
      "|4.90|-0.8322\n",
      "|4.63|-0.1709\n",
      "|2.36|4.8000\n",
      "|4.51|-0.2096\n",
      "|4.61|-0.2087\n",
      "|4.72|-0.2075\n",
      "Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.209   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.045    |\n",
      "|    n_updates        | 1330     |\n",
      "----------------------------------\n",
      "|4.54|-1.4367\n",
      "|4.53|-0.8183\n",
      "|4.65|-1.1918\n",
      "|4.56|-1.4332\n",
      "|4.63|-0.9558\n",
      "|4.58|-1.4894\n",
      "|4.66|-1.2369\n",
      "|4.58|-0.1066\n",
      "|4.59|-1.0018\n",
      "|4.65|-0.3137\n",
      "|4.63|-1.0691\n",
      "|4.55|-1.6321\n",
      "|4.62|-0.2649\n",
      "|4.50|-0.1891\n",
      "|4.55|0.6001\n",
      "|4.59|-0.2089\n",
      "|4.64|-0.2076\n",
      "|4.67|-1.1080\n",
      "|4.64|-0.7767\n",
      "|4.58|-0.9731\n",
      "|4.59|-0.6588\n",
      "|4.58|-0.2153\n",
      "|4.66|-0.2101\n",
      "|4.61|-1.8793\n",
      "|4.64|-0.1707\n",
      "|4.57|-0.2093\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -0.91    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1609     |\n",
      "|    total timesteps  | 32626    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0476   |\n",
      "|    n_updates        | 1450     |\n",
      "----------------------------------\n",
      "|4.64|-1.1980\n",
      "|4.60|-0.7003\n",
      "|4.60|-1.0753\n",
      "|4.59|-2.1503\n",
      "|4.65|-0.0951\n",
      "|4.69|-1.3247\n",
      "|4.61|-1.0334\n",
      "|4.58|-1.9285\n",
      "|4.66|2.7727\n",
      "|4.57|-0.5869\n",
      "|4.68|-0.5219\n",
      "|4.61|-0.1905\n",
      "|4.59|-0.5632\n",
      "|4.59|-2.5327\n",
      "|4.64|-4.3404\n",
      "|4.66|0.5892\n",
      "|4.67|-2.9926\n",
      "|4.60|-0.3471\n",
      "|4.58|-0.2071\n",
      "|4.59|-0.1602\n",
      "|4.58|-1.6116\n",
      "|4.59|-3.0071\n",
      "|4.56|-1.5468\n",
      "|2.27|4.7901\n",
      "|4.60|-0.2092\n",
      "|4.47|-0.2077\n",
      "|4.53|-0.2102\n",
      "Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.209   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0418   |\n",
      "|    n_updates        | 1570     |\n",
      "----------------------------------\n",
      "|4.55|0.6222\n",
      "|4.55|-1.1027\n",
      "|4.53|-1.1146\n",
      "|4.55|-0.1423\n",
      "|4.61|-1.0624\n",
      "|4.56|-0.2673\n",
      "|4.61|-3.0888\n",
      "|4.70|-1.8142\n",
      "|4.61|-1.0385\n",
      "|4.60|-2.3846\n",
      "|4.67|-2.4681\n",
      "|4.59|-3.1832\n",
      "|4.58|-1.9502\n",
      "|4.61|-0.7621\n",
      "|4.53|-0.7073\n",
      "|4.69|2.5250\n",
      "|4.63|-0.6821\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -0.839   |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1809     |\n",
      "|    total timesteps  | 36717    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0472   |\n",
      "|    n_updates        | 1650     |\n",
      "----------------------------------\n",
      "|4.62|4.8689\n",
      "|4.54|0.1959\n",
      "|4.56|1.3700\n",
      "|4.62|6.0936\n",
      "|4.64|-5.7179\n",
      "|4.65|3.4721\n",
      "|4.65|-0.3134\n",
      "|4.64|-0.0024\n",
      "|4.63|6.1979\n",
      "|4.61|4.9968\n",
      "|4.55|1.3268\n",
      "|4.61|-2.2193\n",
      "|4.63|-1.6451\n",
      "|4.52|1.3726\n",
      "|4.65|3.4841\n",
      "|4.66|-2.4231\n",
      "|4.59|-2.0235\n",
      "|4.49|-1.7250\n",
      "|4.71|-1.7188\n",
      "|4.71|-2.6596\n",
      "|4.61|0.0862\n",
      "|4.66|0.2244\n",
      "|4.66|0.0904\n",
      "|4.68|0.0996\n",
      "|4.67|-0.3925\n",
      "|4.75|0.5800\n",
      "|4.55|0.0346\n",
      "|4.63|-0.2093\n",
      "|4.63|-0.8319\n",
      "|4.65|-6.7423\n",
      "|4.62|-0.3412\n",
      "|4.56|0.0893\n",
      "|2.34|5.0916\n",
      "|4.64|0.0811\n",
      "|4.57|-0.2095\n",
      "|4.54|-0.2083\n",
      "Eval num_timesteps=40000, episode_reward=-0.11 +/- 0.14\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.112   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0403   |\n",
      "|    n_updates        | 1820     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|4.61|0.8123\n",
      "|4.54|-1.9896\n",
      "|4.65|-8.8257\n",
      "|4.58|-2.6243\n",
      "|4.60|4.6466\n",
      "|4.58|-3.5235\n",
      "|4.66|-0.8114\n",
      "|4.53|-0.8006\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -0.528   |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2010     |\n",
      "|    total timesteps  | 40808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0459   |\n",
      "|    n_updates        | 1850     |\n",
      "----------------------------------\n",
      "|4.62|0.3328\n",
      "|4.57|0.6390\n",
      "|4.56|0.8243\n",
      "|4.53|-9.4929\n",
      "|4.61|-4.8914\n",
      "|4.48|-2.0224\n",
      "|4.71|4.9527\n",
      "|4.54|1.7135\n",
      "|4.68|0.9772\n",
      "|4.58|2.4233\n",
      "|4.65|3.6034\n",
      "|4.57|0.7393\n",
      "|4.60|-0.2729\n",
      "|4.64|8.7598\n",
      "|4.58|0.6152\n",
      "|4.53|-1.4068\n",
      "|4.49|-0.3420\n",
      "|4.62|-0.3678\n",
      "|4.63|0.6663\n",
      "|4.56|0.6453\n",
      "|4.65|0.5258\n",
      "|4.59|-0.0276\n",
      "|4.61|-0.3544\n",
      "|4.65|0.0745\n",
      "|4.57|-0.8182\n",
      "|4.58|-0.2777\n",
      "|4.56|0.4197\n",
      "|4.63|0.0704\n",
      "|4.64|1.7995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|4.58|0.0409\n",
      "|4.61|1.6719\n",
      "|4.60|-1.2052\n",
      "|4.64|0.6902\n",
      "|4.57|-0.0212\n",
      "|4.56|-7.1379\n",
      "|4.55|-0.8017\n",
      "|4.66|4.0313\n",
      "|4.66|3.0200\n",
      "|4.54|-2.1012\n",
      "|4.53|5.9365\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -0.0877  |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2193     |\n",
      "|    total timesteps  | 44848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0432   |\n",
      "|    n_updates        | 2050     |\n",
      "----------------------------------\n",
      "|4.57|0.4288\n",
      "|2.37|6.1746\n",
      "|4.59|2.1915\n",
      "|4.58|0.3553\n",
      "|4.57|5.5728\n",
      "Eval num_timesteps=45000, episode_reward=2.71 +/- 2.16\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | 2.71     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0416   |\n",
      "|    n_updates        | 2060     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|4.71|0.8456\n",
      "|4.60|-6.2819\n",
      "|4.66|0.8398\n",
      "|4.63|0.0448\n",
      "|4.57|1.6319\n",
      "|4.56|5.4640\n",
      "|4.58|-2.8855\n",
      "|4.72|-0.6071\n",
      "|4.59|0.0272\n",
      "|4.66|0.0202\n",
      "|4.57|0.0816\n",
      "done\n",
      "|4.14|41.4536\n",
      "|4.54|2.7816\n",
      "|4.59|-2.2525\n",
      "|4.56|1.0435\n",
      "|4.60|-1.9874\n",
      "|4.65|-0.2923\n",
      "|4.63|-1.7245\n",
      "|4.61|1.4479\n",
      "|4.67|-1.5864\n",
      "|4.69|7.4645\n",
      "|4.66|-0.1869\n",
      "|4.61|0.0872\n",
      "|4.62|-0.5590\n",
      "|4.50|-2.8788\n",
      "|4.71|5.5405\n",
      "|4.57|-3.3592\n",
      "|4.55|-1.9577\n",
      "|4.59|6.7707\n",
      "|4.58|-0.7745\n",
      "|4.61|-2.8381\n",
      "|4.61|0.2000\n",
      "|4.65|0.3302\n",
      "|4.61|-0.2088\n",
      "|4.55|-3.1484\n",
      "|4.73|3.4871\n",
      "|4.62|4.3032\n",
      "|4.71|5.0702\n",
      "|4.61|3.5451\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 0.638    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2394     |\n",
      "|    total timesteps  | 48929    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0406   |\n",
      "|    n_updates        | 2250     |\n",
      "----------------------------------\n",
      "|4.63|4.6797\n",
      "|4.62|5.2575\n",
      "|4.65|4.7790\n",
      "|4.64|2.6216\n",
      "|4.57|-1.6859\n",
      "|4.62|-1.4933\n",
      "|4.66|2.9471\n",
      "|4.64|6.0775\n",
      "|4.56|6.8942\n",
      "|4.57|4.3977\n",
      "|2.73|3.7459\n",
      "|4.63|-0.2100\n",
      "|4.62|-0.2103\n",
      "|4.59|-0.3066\n",
      "Eval num_timesteps=50000, episode_reward=-0.24 +/- 0.05\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.242   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.04     |\n",
      "|    n_updates        | 2310     |\n",
      "----------------------------------\n",
      "|4.56|4.8370\n",
      "|4.51|6.5825\n",
      "|4.68|1.9567\n",
      "|4.67|6.6060\n",
      "|4.65|8.0727\n",
      "|4.60|-0.3943\n",
      "|4.70|3.9069\n",
      "|4.60|7.4519\n",
      "|4.64|5.8935\n",
      "done\n",
      "|3.86|41.8443\n",
      "|4.61|5.0978\n",
      "done\n",
      "|4.06|41.1443\n",
      "done\n",
      "|3.59|43.0717\n",
      "|4.49|-4.0861\n",
      "|4.67|5.8117\n",
      "|5.00|8.2202\n",
      "|4.72|-0.9610\n",
      "|5.03|6.0187\n",
      "|5.18|-0.4233\n",
      "|4.63|-3.9195\n",
      "|5.10|-1.3023\n",
      "done\n",
      "|3.70|42.4492\n",
      "|4.67|-2.7629\n",
      "|5.03|-9.4928\n",
      "|4.65|-7.9627\n",
      "|4.62|-6.9428\n",
      "done\n",
      "|4.32|40.8811\n",
      "|4.72|1.2638\n",
      "done\n",
      "|4.19|41.2995\n",
      "done\n",
      "|3.94|42.3306\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 4.36     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2593     |\n",
      "|    total timesteps  | 52905    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0432   |\n",
      "|    n_updates        | 2450     |\n",
      "----------------------------------\n",
      "done\n",
      "|3.89|41.6302\n",
      "done\n",
      "|3.67|41.9200\n",
      "done\n",
      "|3.15|42.8961\n",
      "done\n",
      "|3.44|43.5386\n",
      "done\n",
      "|3.50|43.2206\n",
      "done\n",
      "|4.19|42.6191\n",
      "done\n",
      "|3.57|42.7648\n",
      "done\n",
      "|3.72|42.8912\n",
      "|5.76|3.7178\n",
      "|4.96|5.2666\n",
      "|4.98|5.4332\n",
      "done\n",
      "|4.64|41.8763\n",
      "done\n",
      "|3.50|42.6843\n",
      "done\n",
      "|3.62|42.5792\n",
      "done\n",
      "|2.89|44.2718\n",
      "done\n",
      "|3.81|42.2468\n",
      "|4.72|-2.1064\n",
      "|4.70|4.5717\n",
      "done\n",
      "|3.31|43.8797\n",
      "done\n",
      "|3.00|44.7875\n",
      "done\n",
      "|3.39|44.0060\n",
      "done\n",
      "|3.83|43.0473\n",
      "|4.67|-1.2215\n",
      "done\n",
      "|3.20|43.6516\n",
      "done\n",
      "|3.21|43.9504\n",
      "done\n",
      "|2.80|44.4857\n",
      "|1.85|4.8909\n",
      "done\n",
      "|3.07|43.8849\n",
      "done\n",
      "|3.10|43.9650\n",
      "done\n",
      "|3.30|43.6550\n",
      "Eval num_timesteps=55000, episode_reward=43.83 +/- 0.13\n",
      "Episode length: 67.33 +/- 1.25\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 67.3     |\n",
      "|    mean_reward      | 43.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0527   |\n",
      "|    n_updates        | 2590     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "done\n",
      "|3.06|44.1806\n",
      "done\n",
      "|3.41|43.6573\n",
      "done\n",
      "|3.09|43.9994\n",
      "done\n",
      "|3.21|43.6109\n",
      "done\n",
      "|3.59|41.9973\n",
      "done\n",
      "|3.37|42.5828\n",
      "done\n",
      "|3.51|42.8762\n",
      "done\n",
      "|2.93|44.5765\n",
      "done\n",
      "|3.56|42.3724\n",
      "done\n",
      "|3.86|42.5347\n",
      "done\n",
      "|3.14|43.4430\n",
      "done\n",
      "|3.71|42.3488\n",
      "done\n",
      "|3.37|43.4496\n",
      "done\n",
      "|3.48|42.8697\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.9     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2752     |\n",
      "|    total timesteps  | 55999    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0527   |\n",
      "|    n_updates        | 2650     |\n",
      "----------------------------------\n",
      "done\n",
      "|2.95|43.7324\n",
      "done\n",
      "|3.33|43.4492\n",
      "done\n",
      "|3.13|44.3136\n",
      "done\n",
      "|3.22|43.1383\n",
      "done\n",
      "|3.13|43.6471\n",
      "done\n",
      "|3.19|43.5524\n",
      "|4.65|-7.9036\n",
      "|4.86|-3.4238\n",
      "done\n",
      "|3.68|43.4800\n",
      "done\n",
      "|3.79|42.4910\n",
      "done\n",
      "|3.80|42.7191\n",
      "done\n",
      "|3.60|43.6966\n",
      "done\n",
      "|3.59|43.4913\n",
      "done\n",
      "|3.68|42.8839\n",
      "done\n",
      "|4.23|41.7281\n",
      "done\n",
      "|3.58|42.6290\n",
      "done\n",
      "|3.95|42.6605\n",
      "done\n",
      "|3.42|42.9545\n",
      "done\n",
      "|3.74|42.7488\n",
      "done\n",
      "|3.51|42.9901\n",
      "done\n",
      "|2.86|44.2749\n",
      "done\n",
      "|3.48|42.8609\n",
      "done\n",
      "|3.47|43.3426\n",
      "done\n",
      "|3.99|42.2611\n",
      "done\n",
      "|3.97|42.0289\n",
      "done\n",
      "|3.66|43.0489\n",
      "done\n",
      "|4.36|42.0324\n",
      "done\n",
      "|3.74|42.5665\n",
      "done\n",
      "|3.90|42.0853\n",
      "done\n",
      "|4.10|41.6679\n",
      "done\n",
      "|3.99|41.4250\n",
      "done\n",
      "|4.33|41.3580\n",
      "done\n",
      "|3.56|42.7113\n",
      "done\n",
      "|3.90|41.8084\n",
      "done\n",
      "|3.67|43.2112\n",
      "done\n",
      "|3.85|42.2509\n",
      "|4.77|4.6117\n",
      "done\n",
      "|5.24|42.9186\n",
      "done\n",
      "|4.54|42.4857\n",
      "done\n",
      "|3.53|42.7563\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.9     |\n",
      "|    ep_rew_mean      | 33       |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2904     |\n",
      "|    total timesteps  | 59179    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.074    |\n",
      "|    n_updates        | 2850     |\n",
      "----------------------------------\n",
      "done\n",
      "|3.89|42.2427\n",
      "done\n",
      "|3.45|43.3742\n",
      "done\n",
      "|3.56|43.3155\n",
      "done\n",
      "|3.79|42.2281\n",
      "done\n",
      "|4.28|42.5695\n",
      "done\n",
      "|4.03|41.5016\n",
      "done\n",
      "|4.14|42.5393\n",
      "done\n",
      "|3.40|43.4480\n",
      "done\n",
      "|3.91|42.5777\n",
      "done\n",
      "|3.67|43.0323\n",
      "|1.70|7.0121\n",
      "done\n",
      "|3.46|43.3057\n",
      "done\n",
      "|3.73|43.0256\n",
      "done\n",
      "|3.62|44.4354\n",
      "Eval num_timesteps=60000, episode_reward=43.59 +/- 0.61\n",
      "Episode length: 74.33 +/- 0.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 74.3     |\n",
      "|    mean_reward      | 43.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0727   |\n",
      "|    n_updates        | 2910     |\n",
      "----------------------------------\n",
      "done\n",
      "|3.94|41.0718\n",
      "done\n",
      "|3.91|42.3700\n",
      "done\n",
      "|3.14|43.8203\n",
      "done\n",
      "|2.99|44.3512\n",
      "done\n",
      "|3.61|42.9328\n",
      "done\n",
      "|3.67|42.0050\n",
      "done\n",
      "|4.44|40.9790\n",
      "|4.56|-4.3784\n",
      "done\n",
      "|3.67|43.2885\n",
      "done\n",
      "|3.68|42.4282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "|3.28|43.5265\n",
      "done\n",
      "|4.13|42.0221\n",
      "done\n",
      "|3.49|43.2596\n",
      "done\n",
      "|3.64|42.7280\n",
      "done\n",
      "|3.80|42.2695\n",
      "done\n",
      "|4.14|41.1319\n",
      "done\n",
      "|3.65|42.4250\n",
      "done\n",
      "|4.27|41.1870\n",
      "done\n",
      "|3.29|43.8907\n",
      "done\n",
      "|3.73|43.8314\n",
      "done\n",
      "|3.43|43.1081\n",
      "done\n",
      "|3.60|42.2739\n",
      "done\n",
      "|3.84|42.8212\n",
      "done\n",
      "|3.63|42.6690\n",
      "done\n",
      "|3.76|42.8131\n",
      "done\n",
      "|3.34|43.6683\n",
      "done\n",
      "|3.33|43.0792\n",
      "done\n",
      "|3.53|42.2815\n",
      "done\n",
      "|3.88|42.4238\n",
      "done\n",
      "|3.91|41.8064\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.7     |\n",
      "|    ep_rew_mean      | 40.7     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 3065     |\n",
      "|    total timesteps  | 62382    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.077    |\n",
      "|    n_updates        | 3050     |\n",
      "----------------------------------\n",
      "done\n",
      "|3.66|42.9578\n",
      "done\n",
      "|4.43|41.1653\n",
      "done\n",
      "|3.63|42.8009\n",
      "done\n",
      "|3.81|42.3453\n",
      "done\n",
      "|3.58|43.7041\n",
      "done\n",
      "|3.36|43.9248\n",
      "done\n",
      "|3.61|42.4122\n",
      "done\n",
      "|4.14|41.7725\n",
      "done\n",
      "|3.55|43.0211\n",
      "done\n",
      "|3.93|42.4265\n",
      "done\n",
      "|3.63|42.5793\n",
      "done\n",
      "|3.70|42.6995\n",
      "done\n",
      "|4.19|41.0559\n",
      "done\n",
      "|3.66|42.8880\n",
      "done\n",
      "|4.32|42.3854\n",
      "done\n",
      "|3.05|44.4243\n",
      "done\n",
      "|4.05|41.9523\n",
      "done\n",
      "|3.39|43.5984\n",
      "done\n",
      "|3.87|41.8309\n",
      "done\n",
      "|3.34|42.9649\n",
      "done\n",
      "|3.71|43.3212\n",
      "done\n",
      "|4.17|42.1095\n",
      "|4.66|5.9455\n",
      "done\n",
      "|3.01|44.2110\n",
      "|4.73|-2.0249\n",
      "|4.64|-4.1832\n",
      "done\n",
      "|3.29|42.7544\n",
      "done\n",
      "|2.94|43.8058\n",
      "done\n",
      "|3.48|42.7697\n",
      "done\n",
      "|2.88|43.8930\n",
      "done\n",
      "|3.39|42.6152\n",
      "done\n",
      "|3.12|43.3304\n",
      "|2.15|7.2052\n",
      "done\n",
      "|3.10|43.4127\n",
      "done\n",
      "|2.86|43.8857\n",
      "done\n",
      "|3.12|44.2715\n",
      "Eval num_timesteps=65000, episode_reward=43.86 +/- 0.35\n",
      "Episode length: 65.67 +/- 2.62\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 65.7     |\n",
      "|    mean_reward      | 43.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0938   |\n",
      "|    n_updates        | 3220     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "done\n",
      "|3.25|43.0182\n",
      "done\n",
      "|3.51|42.9143\n",
      "Evaluating...\n",
      "|0.03|0.0000\n",
      "1 -> [0.   0.81] 0.6\n",
      "1 -> [0.   0.66] 0.97\n",
      "2 -> [0.2  0.66] -0.71\n",
      "1 -> [0.2  0.51] 1.03\n",
      "1 -> [0.2  0.35] 1.47\n",
      "2 -> [0.4  0.35] -0.66\n",
      "2 -> [0.6  0.35] -0.1\n",
      "3 -> [0.6  0.55] -0.1\n",
      "3 -> [0.6  0.75] -0.1\n",
      "3 -> [0.6  0.95] -0.1\n",
      "0 -> [0.4  0.95] -0.1\n",
      "0 -> [0.2  0.95] -0.1\n",
      "1 -> [0.2  0.75] -0.1\n",
      "1 -> [0.2 0.6] 0.91\n",
      "1 -> [0.2  0.44] 1.23\n",
      "2 -> [0.4  0.44] -0.46\n",
      "3 -> [0.4  0.64] -0.1\n",
      "3 -> [0.4  0.84] -0.1\n",
      "3 -> [0.4 1. ] -0.1\n",
      "0 -> [0.2 1. ] -0.1\n",
      "0 -> [0.03 1.  ] -0.1\n",
      "1 -> [0.03 0.85] 0.49\n",
      "1 -> [0.03 0.68] 0.97\n",
      "1 -> [0.03 0.53] 1.19\n",
      "2 -> [0.23 0.53] -0.79\n",
      "1 -> [0.23 0.39] 1.22\n",
      "2 -> [0.43 0.39] -0.43\n",
      "3 -> [0.43 0.59] -0.1\n",
      "3 -> [0.43 0.79] -0.1\n",
      "3 -> [0.43 0.99] -0.1\n",
      "0 -> [0.23 0.99] -0.1\n",
      "0 -> [0.04 0.99] -0.1\n",
      "1 -> [0.04 0.85] 0.47\n",
      "1 -> [0.04 0.7 ] 0.77\n",
      "1 -> [0.04 0.54] 1.25\n",
      "2 -> [0.24 0.54] -0.75\n",
      "1 -> [0.24 0.41] 1.09\n",
      "2 -> [0.44 0.41] -0.35\n",
      "3 -> [0.44 0.61] -0.1\n",
      "3 -> [0.44 0.8 ] -0.1\n",
      "3 -> [0.44 1.  ] -0.1\n",
      "0 -> [0.24 1.  ] -0.1\n",
      "0 -> [0.06 1.  ] -0.1\n",
      "1 -> [0.06 0.83] 0.53\n",
      "1 -> [0.06 0.7 ] 0.76\n",
      "1 -> [0.06 0.55] 1.14\n",
      "2 -> [0.26 0.55] -0.74\n",
      "1 -> [0.26 0.41] 1.11\n",
      "2 -> [0.46 0.41] -0.1\n",
      "3 -> [0.46 0.61] -0.1\n",
      "3 -> [0.46 0.81] -0.1\n",
      "3 -> [0.46 1.  ] -0.1\n",
      "0 -> [0.28 1.  ] -0.1\n",
      "0 -> [0.08 1.  ] -0.1\n",
      "1 -> [0.08 0.82] 0.56\n",
      "1 -> [0.08 0.66] 0.93\n",
      "1 -> [0.08 0.5 ] 1.22\n",
      "2 -> [0.28 0.5 ] -0.75\n",
      "1 -> [0.28 0.35] 1.3\n",
      "2 -> [0.48 0.35] -0.34\n",
      "3 -> [0.48 0.55] -0.1\n",
      "3 -> [0.48 0.75] -0.1\n",
      "3 -> [0.48 0.95] -0.1\n",
      "0 -> [0.28 0.95] -0.1\n",
      "0 -> [0.09 0.95] -0.1\n",
      "1 -> [0.09 0.78] 0.62\n",
      "done\n",
      "1 -> [0.09 0.62] 30.98\n",
      "Evaluated 43.527106511592876\n",
      "Training [0.01, 64, 0, 5000, 10] 1\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "|3.12|43.5271\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to tlog/dqn_evals_6/DQN_LR0.010_H1_64_H2_0_BS_5000_GS_10_1_1\n",
      "|4.52|-11.4415\n",
      "|4.66|-14.4082\n",
      "|4.63|-16.3391\n",
      "|4.73|-15.0496\n",
      "|4.63|-12.0731\n",
      "|4.58|-8.9178\n",
      "|4.65|-5.7506\n",
      "|4.72|-10.2483\n",
      "|4.63|-9.2422\n",
      "|4.66|-9.1550\n",
      "|4.67|-10.9364\n",
      "|4.72|-14.4518\n",
      "|4.58|-11.5687\n",
      "|4.74|-10.5847\n",
      "|4.58|-8.0372\n",
      "|4.61|-15.8223\n",
      "|4.64|-9.9906\n",
      "|4.77|-6.2376\n",
      "|5.04|-7.9384\n",
      "|5.18|-7.8064\n",
      "|4.69|-6.9514\n",
      "|4.90|-12.8282\n",
      "|4.67|-6.7731\n",
      "|4.73|-10.4053\n",
      "|4.56|-1.5355\n",
      "|4.63|-14.6692\n",
      "|4.51|-3.7658\n",
      "|4.59|-6.3727\n",
      "|4.62|-6.5573\n",
      "|4.56|-9.9522\n",
      "|4.68|-8.1424\n",
      "|4.57|-7.3722\n",
      "|4.66|-2.3571\n",
      "|4.66|-8.4534\n",
      "|4.63|-8.1213\n",
      "|4.62|-8.0239\n",
      "|4.60|-17.2293\n",
      "|4.61|-9.1039\n",
      "|5.20|-5.1639\n",
      "|4.79|-4.6518\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | -9.36    |\n",
      "|    exploration rate | 0.852    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total timesteps  | 4040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0467   |\n",
      "|    n_updates        | 50       |\n",
      "----------------------------------\n",
      "|5.04|-16.0603\n",
      "|4.62|-4.1935\n",
      "|4.52|-4.9024\n",
      "|4.62|-4.8018\n",
      "|4.55|-17.8123\n",
      "|4.65|-12.9642\n",
      "|5.22|-6.4371\n",
      "|4.81|-13.5157\n",
      "|4.78|-10.8998\n",
      "|2.37|-0.9436\n",
      "|4.94|-0.2089\n",
      "|4.99|-0.4030\n",
      "|4.82|-0.2075\n",
      "Eval num_timesteps=5000, episode_reward=-0.27 +/- 0.09\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.273   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0549   |\n",
      "|    n_updates        | 100      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|4.94|-9.5063\n",
      "|4.76|-11.4297\n",
      "|4.91|-7.9739\n",
      "|5.46|-12.6477\n",
      "|4.96|-11.1654\n",
      "|4.96|-3.6653\n",
      "|4.59|-11.8947\n",
      "|4.73|-9.3607\n",
      "|4.68|-7.9905\n",
      "|5.03|-8.3754\n",
      "|5.78|-10.4882\n",
      "|5.33|-14.4284\n",
      "|5.42|-8.4103\n",
      "|4.78|-7.3773\n",
      "|4.58|-0.0339\n",
      "|5.07|-13.1283\n",
      "|4.79|-9.0387\n",
      "|4.59|-2.9064\n",
      "|5.06|-7.8031\n",
      "|4.87|-5.5701\n",
      "|4.58|-4.1714\n",
      "|4.88|-9.6147\n",
      "|5.10|-1.7986\n",
      "|5.40|-3.2065\n",
      "|4.58|-10.3613\n",
      "|5.19|-8.4426\n",
      "|4.88|-13.3403\n",
      "|4.71|-4.8695\n",
      "|4.62|-6.1291\n",
      "|4.73|-8.9952\n",
      "|4.64|-3.2390\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -8.93    |\n",
      "|    exploration rate | 0.703    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 399      |\n",
      "|    total timesteps  | 8131     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0362   |\n",
      "|    n_updates        | 250      |\n",
      "----------------------------------\n",
      "|4.66|-12.5787\n",
      "|4.59|-8.4979\n",
      "|4.79|-1.5763\n",
      "|4.59|-9.1656\n",
      "|4.70|-9.9999\n",
      "|4.66|-5.5804\n",
      "|4.62|-8.0921\n",
      "|4.77|-5.5585\n",
      "|4.72|-2.4153\n",
      "|4.86|-10.0879\n",
      "|4.68|-5.5104\n",
      "|4.69|-5.9551\n",
      "|4.69|-8.6778\n",
      "|4.73|-6.3803\n",
      "|4.62|-12.2363\n",
      "|4.62|-10.0044\n",
      "|4.56|-3.7565\n",
      "|4.62|1.5508\n",
      "|2.50|-0.4995\n",
      "|5.07|-0.3512\n",
      "|4.75|-0.5721\n",
      "|4.91|-0.1613\n",
      "Eval num_timesteps=10000, episode_reward=-0.36 +/- 0.17\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.362   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.635    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0414   |\n",
      "|    n_updates        | 350      |\n",
      "----------------------------------\n",
      "|5.20|-14.1353\n",
      "|5.36|2.4550\n",
      "|4.71|-4.9028\n",
      "|4.78|-11.0077\n",
      "|4.99|-4.7986\n",
      "|4.73|-10.0336\n",
      "|4.86|-6.0014\n",
      "|4.73|-8.1037\n",
      "|4.67|-4.4012\n",
      "|4.57|-5.7308\n",
      "|4.63|-11.7652\n",
      "|4.66|-7.2215\n",
      "|4.56|-11.8912\n",
      "|4.63|-8.6017\n",
      "|4.55|-5.5768\n",
      "|4.61|-5.2203\n",
      "|4.74|-9.8671\n",
      "|4.58|-5.6072\n",
      "|4.67|-5.6089\n",
      "|4.61|-6.5947\n",
      "|4.57|-10.0241\n",
      "|4.70|-9.5247\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -7.88    |\n",
      "|    exploration rate | 0.553    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 605      |\n",
      "|    total timesteps  | 12222    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 450      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|4.65|-4.1343\n",
      "|4.59|-10.4578\n",
      "|4.54|-6.6494\n",
      "|4.59|-7.3950\n",
      "|4.82|-7.4351\n",
      "|4.72|-6.7792\n",
      "|4.64|-7.0424\n",
      "|4.65|-9.2504\n",
      "|4.56|-7.2919\n",
      "|4.55|-7.2983\n",
      "|4.58|-0.8660\n",
      "|4.57|-9.4816\n",
      "|4.72|-8.5030\n",
      "|4.60|-5.7572\n",
      "|4.73|-3.0590\n",
      "|4.66|0.3039\n",
      "|4.59|-6.2074\n",
      "|4.58|-2.7984\n",
      "|4.73|-13.5056\n",
      "|4.53|-5.3350\n",
      "|4.53|-8.1924\n",
      "|4.55|-2.8075\n",
      "|4.63|-6.2614\n",
      "|4.59|-7.6779\n",
      "|4.67|-4.7437\n",
      "|4.59|-11.0409\n",
      "|4.54|-8.9649\n",
      "|2.37|0.5020\n",
      "|4.70|-0.2094\n",
      "|4.64|-0.2077\n",
      "|4.64|-0.2094\n",
      "Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.209   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0426   |\n",
      "|    n_updates        | 590      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|4.64|-5.2075\n",
      "|4.63|-5.6215\n",
      "|4.60|-12.5474\n",
      "|4.66|-4.6565\n",
      "|4.60|-4.1280\n",
      "|4.57|-5.0877\n",
      "|4.58|-9.2034\n",
      "|4.57|0.0167\n",
      "|4.60|-8.4059\n",
      "|4.61|-3.2375\n",
      "|4.52|-6.7444\n",
      "|4.70|-2.1561\n",
      "|4.57|-6.6058\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -6.84    |\n",
      "|    exploration rate | 0.404    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 806      |\n",
      "|    total timesteps  | 16313    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.168    |\n",
      "|    n_updates        | 650      |\n",
      "----------------------------------\n",
      "|4.56|-7.6356\n",
      "|4.58|-7.5197\n",
      "|4.64|-7.3032\n",
      "|4.63|-4.6790\n",
      "|4.60|-1.4203\n",
      "|4.71|-4.0690\n",
      "|4.57|-2.4075\n",
      "|4.65|-2.5728\n",
      "|4.73|-4.5853\n",
      "|4.62|-1.1486\n",
      "|4.75|-9.1246\n",
      "|4.63|-1.7068\n",
      "|4.54|-1.9050\n",
      "|4.54|-4.3413\n",
      "|4.73|-7.4882\n",
      "|4.64|-5.1531\n",
      "|4.62|-6.5091\n",
      "|4.62|-2.6614\n",
      "|4.64|-2.5095\n",
      "|4.54|-5.6619\n",
      "|4.70|-3.2795\n",
      "|4.66|-5.2620\n",
      "|4.67|-0.9683\n",
      "|4.61|-1.4346\n",
      "|4.64|-2.1781\n",
      "|4.60|-3.0334\n",
      "|4.66|-1.0299\n",
      "|5.04|-5.6034\n",
      "|4.61|-6.6474\n",
      "|4.61|-1.3421\n",
      "|4.52|-0.4315\n",
      "|4.57|-3.4495\n",
      "|4.56|-4.9636\n",
      "|4.58|-2.2105\n",
      "|4.53|-2.5622\n",
      "|4.58|-0.6160\n",
      "|2.33|3.4410\n",
      "|4.50|-0.2091\n",
      "|4.55|-0.3834\n",
      "|4.58|-0.1567\n",
      "Eval num_timesteps=20000, episode_reward=-0.25 +/- 0.10\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -0.25    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.269    |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0421   |\n",
      "|    n_updates        | 840      |\n",
      "----------------------------------\n",
      "|4.50|-7.0747\n",
      "|4.54|-3.6112\n",
      "|4.64|-5.0142\n",
      "|4.58|-4.4885\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -5.56    |\n",
      "|    exploration rate | 0.254    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1007     |\n",
      "|    total timesteps  | 20404    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 850      |\n",
      "----------------------------------\n",
      "|4.63|-4.6998\n",
      "|4.51|-2.7922\n",
      "|4.58|-5.2019\n",
      "|4.70|-1.3601\n",
      "|4.64|-1.5236\n",
      "|4.58|-4.9959\n",
      "|4.53|-4.1609\n",
      "|4.49|-4.3349\n",
      "|4.56|-0.5013\n",
      "|4.59|-3.6645\n",
      "|4.56|-1.5714\n",
      "|4.62|-1.4668\n",
      "|4.59|-2.5653\n",
      "|4.56|-2.0038\n",
      "|4.60|-5.0903\n",
      "|4.56|-0.6461\n",
      "|4.63|-0.8850\n",
      "|4.58|-0.4429\n",
      "|4.64|-2.3294\n",
      "|4.58|-2.8783\n",
      "|4.58|-2.0159\n",
      "|4.59|-1.0128\n",
      "|4.57|-1.0607\n",
      "|4.50|-2.5814\n",
      "|4.66|-1.9592\n",
      "|4.57|-2.1334\n",
      "|4.49|-0.8413\n",
      "|4.63|-4.3965\n",
      "|4.63|-2.2859\n",
      "|4.62|-1.1560\n",
      "|4.60|-2.0064\n",
      "|4.53|1.1270\n",
      "|4.58|-0.6492\n",
      "|4.52|-2.7511\n",
      "|4.50|-0.6275\n",
      "|4.60|-2.3859\n",
      "|4.56|-2.5285\n",
      "|4.56|-2.6606\n",
      "|4.60|-1.6840\n",
      "|4.63|-1.7103\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -3.63    |\n",
      "|    exploration rate | 0.107    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1190     |\n",
      "|    total timesteps  | 24444    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.067    |\n",
      "|    n_updates        | 1050     |\n",
      "----------------------------------\n",
      "|4.66|-2.9855\n",
      "|4.60|-1.3397\n",
      "|4.64|-1.7249\n",
      "|4.52|-0.9338\n",
      "|4.59|-2.7374\n",
      "|2.31|2.8852\n",
      "|4.51|-0.7677\n",
      "|4.61|-3.0121\n",
      "|4.51|-1.0166\n",
      "Eval num_timesteps=25000, episode_reward=-1.60 +/- 1.00\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -1.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.0866   |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0465   |\n",
      "|    n_updates        | 1080     |\n",
      "----------------------------------\n",
      "|4.66|-0.8803\n",
      "|4.57|-3.9448\n",
      "|4.60|-4.6444\n",
      "|4.62|-0.3709\n",
      "|4.56|-0.9467\n",
      "|4.64|-1.2476\n",
      "|4.63|-0.8701\n",
      "|4.63|0.1875\n",
      "|4.59|-0.3427\n",
      "|4.61|-2.0664\n",
      "|4.49|-1.1767\n",
      "|4.63|-0.4132\n",
      "|4.59|-1.7704\n",
      "|4.59|-0.2682\n",
      "|4.56|-1.4482\n",
      "|4.52|-3.4055\n",
      "|4.59|-3.7831\n",
      "|4.60|0.1698\n",
      "|4.54|-0.2835\n",
      "|4.60|-4.3883\n",
      "|4.55|1.3193\n",
      "|4.66|-2.3857\n",
      "|4.62|-0.9686\n",
      "|4.53|-0.8971\n",
      "|4.58|-3.5052\n",
      "|4.52|-0.2238\n",
      "|4.56|-0.7506\n",
      "|4.58|-0.0105\n",
      "|4.59|-0.3007\n",
      "|4.60|-6.1338\n",
      "|4.58|-1.5269\n",
      "|4.64|-4.8600\n",
      "|4.65|-1.9603\n",
      "|4.57|-1.2337\n",
      "|4.51|-2.9518\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -2.15    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1390     |\n",
      "|    total timesteps  | 28535    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0608   |\n",
      "|    n_updates        | 1250     |\n",
      "----------------------------------\n",
      "|4.59|-4.0723\n",
      "|4.64|-12.2212\n",
      "|4.59|0.9011\n",
      "|4.57|-9.7749\n",
      "|4.59|-4.5903\n",
      "|4.62|-3.4213\n",
      "|4.55|-2.2671\n",
      "|4.53|-1.2271\n",
      "|4.64|1.3525\n",
      "|4.59|-1.3181\n",
      "|4.63|-0.1254\n",
      "|4.59|-2.7143\n",
      "|4.54|-4.3469\n",
      "|4.68|-5.9431\n",
      "|2.40|-3.8553\n",
      "|4.56|-9.1105\n",
      "|4.59|-3.0480\n",
      "|4.65|-4.4793\n",
      "Eval num_timesteps=30000, episode_reward=-5.55 +/- 2.59\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | -5.55    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0467   |\n",
      "|    n_updates        | 1330     |\n",
      "----------------------------------\n",
      "|4.61|-2.9609\n",
      "|4.57|-1.7182\n",
      "|4.64|1.6003\n",
      "|4.57|1.0163\n",
      "|4.62|1.9642\n",
      "|4.52|-0.7488\n",
      "|4.61|-2.6263\n",
      "|4.63|-0.0132\n",
      "|4.54|-1.2450\n",
      "|4.65|-7.7023\n",
      "|4.57|-4.5879\n",
      "|4.56|2.0616\n",
      "|4.62|-2.7045\n",
      "|4.60|0.5523\n",
      "|4.55|-6.8944\n",
      "|4.62|-2.3377\n",
      "|4.65|-13.4837\n",
      "|4.61|-5.1568\n",
      "|4.61|-0.8703\n",
      "|4.62|3.5803\n",
      "|4.68|-2.6636\n",
      "|4.62|-1.9891\n",
      "|4.70|4.7337\n",
      "|4.58|-0.0209\n",
      "|4.67|1.1060\n",
      "|4.63|3.2646\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -1.92    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1590     |\n",
      "|    total timesteps  | 32626    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0516   |\n",
      "|    n_updates        | 1450     |\n",
      "----------------------------------\n",
      "|4.61|-0.8820\n",
      "|4.56|2.0769\n",
      "|4.66|2.0168\n",
      "|4.57|0.3362\n",
      "|4.64|0.5267\n",
      "|4.57|0.9293\n",
      "|4.63|-0.9464\n",
      "|4.64|5.2884\n",
      "|4.64|8.7270\n",
      "|4.60|-0.4506\n",
      "|4.60|1.0010\n",
      "|4.60|0.7179\n",
      "|4.59|5.9518\n",
      "|4.64|8.1385\n",
      "|4.59|5.3383\n",
      "|4.58|0.9974\n",
      "|4.61|3.7526\n",
      "|4.55|5.8768\n",
      "|4.59|-0.1023\n",
      "|4.58|7.5712\n",
      "|4.69|4.9675\n",
      "|4.60|1.7774\n",
      "|4.65|1.6714\n",
      "|2.36|-1.3002\n",
      "|4.65|0.7854\n",
      "|4.66|-1.2704\n",
      "|4.60|0.9353\n",
      "Eval num_timesteps=35000, episode_reward=0.15 +/- 1.01\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | 0.15     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0455   |\n",
      "|    n_updates        | 1570     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "|4.56|3.0140\n",
      "|4.63|5.4670\n",
      "|4.57|0.7522\n",
      "|4.56|0.5845\n",
      "|4.63|2.5776\n",
      "|4.67|3.2469\n",
      "|4.62|1.7780\n",
      "|4.57|-2.7319\n",
      "|4.66|-0.2981\n",
      "|4.56|-1.2182\n",
      "|4.63|-2.6640\n",
      "|4.62|-9.1518\n",
      "|4.46|-9.3203\n",
      "|4.61|4.4080\n",
      "|4.56|5.5328\n",
      "|4.63|1.1607\n",
      "|4.69|2.0051\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -0.604   |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1790     |\n",
      "|    total timesteps  | 36717    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0528   |\n",
      "|    n_updates        | 1650     |\n",
      "----------------------------------\n",
      "|4.58|2.5119\n",
      "|4.51|2.1422\n",
      "|4.65|-5.4708\n",
      "|4.60|6.1180\n",
      "|4.59|-1.9026\n",
      "|4.51|3.1362\n",
      "|4.64|1.3596\n",
      "|4.55|2.0217\n",
      "|4.58|1.8651\n",
      "|4.63|1.5648\n",
      "done\n",
      "|4.34|41.3020\n",
      "|4.66|0.3820\n",
      "|4.62|1.4894\n",
      "|4.59|-0.3080\n",
      "|4.57|2.1033\n",
      "|4.63|0.3038\n",
      "|4.67|0.6086\n",
      "|4.68|3.3993\n",
      "|4.58|2.7447\n",
      "|4.57|-1.2496\n",
      "|4.56|3.4233\n",
      "|4.58|2.4848\n",
      "|4.67|2.5528\n",
      "|4.57|4.0129\n",
      "|4.60|5.5075\n",
      "|4.62|3.0242\n",
      "|4.63|2.8565\n",
      "|4.58|1.9291\n",
      "|4.63|0.4512\n",
      "|4.53|-0.4770\n",
      "|4.67|2.0726\n",
      "|4.58|4.0402\n",
      "|2.69|3.2315\n",
      "|4.69|4.1642\n",
      "|4.57|5.4848\n",
      "|4.66|3.5385\n",
      "Eval num_timesteps=40000, episode_reward=4.40 +/- 0.81\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | 4.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0491   |\n",
      "|    n_updates        | 1820     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|4.65|3.4720\n",
      "|4.69|6.5519\n",
      "|4.63|-9.1548\n",
      "|4.53|-9.4929\n",
      "|4.60|2.7292\n",
      "|4.62|4.3095\n",
      "|4.53|-2.6161\n",
      "|4.58|1.1215\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 1.28     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 1991     |\n",
      "|    total timesteps  | 40808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0537   |\n",
      "|    n_updates        | 1850     |\n",
      "----------------------------------\n",
      "|4.75|3.4251\n",
      "|4.59|3.7234\n",
      "|4.58|2.6006\n",
      "|4.57|2.9755\n",
      "|4.67|-0.1866\n",
      "|4.61|-0.5429\n",
      "|4.57|2.8244\n",
      "|4.60|2.8413\n",
      "|4.60|0.4443\n",
      "|4.66|0.3777\n",
      "|4.68|2.8931\n",
      "|4.61|3.4173\n",
      "|4.55|6.5591\n",
      "|4.54|1.8344\n",
      "|4.66|4.8853\n",
      "|4.62|5.0418\n",
      "|4.54|2.3325\n",
      "|4.49|-0.5727\n",
      "|4.65|2.6323\n",
      "|4.57|4.1631\n",
      "|4.61|-8.3491\n",
      "|4.43|1.6376\n",
      "|4.67|4.9299\n",
      "|4.60|7.3319\n",
      "|4.64|1.0777\n",
      "|4.54|0.6121\n",
      "|4.59|4.9501\n",
      "|4.57|3.9464\n",
      "|4.55|1.8896\n",
      "|4.60|2.3082\n",
      "|4.55|3.5209\n",
      "|4.56|3.4528\n",
      "|4.59|-3.1655\n",
      "|4.64|1.2497\n",
      "|4.66|0.8324\n",
      "|4.67|2.2575\n",
      "|4.66|2.6803\n",
      "|4.63|3.4645\n",
      "|4.56|2.9564\n",
      "|4.64|1.0602\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 1.99     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2175     |\n",
      "|    total timesteps  | 44848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0544   |\n",
      "|    n_updates        | 2050     |\n",
      "----------------------------------\n",
      "|4.62|1.2935\n",
      "|2.30|1.5351\n",
      "|4.58|3.1438\n",
      "|4.58|2.9317\n",
      "|4.65|3.0392\n",
      "Eval num_timesteps=45000, episode_reward=3.04 +/- 0.09\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | 3.04     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0552   |\n",
      "|    n_updates        | 2060     |\n",
      "----------------------------------\n",
      "|4.67|2.3009\n",
      "|4.68|2.3119\n",
      "|4.63|2.1714\n",
      "|4.62|1.2980\n",
      "|4.55|2.6606\n",
      "|4.54|1.2691\n",
      "|4.58|1.3576\n",
      "|4.62|2.0552\n",
      "|4.60|3.1481\n",
      "|4.60|1.7044\n",
      "|4.50|2.9723\n",
      "|4.58|1.5926\n",
      "|4.65|3.4904\n",
      "|4.62|0.9271\n",
      "|4.57|0.1035\n",
      "|4.60|2.3413\n",
      "|4.60|3.0082\n",
      "|4.56|2.7197\n",
      "|4.61|1.9763\n",
      "|4.59|3.2643\n",
      "|4.56|4.7143\n",
      "|4.62|5.4363\n",
      "|4.55|6.5889\n",
      "|4.63|2.8570\n",
      "|4.67|3.2090\n",
      "|4.67|2.0342\n",
      "|4.53|-1.0201\n",
      "|4.66|3.8085\n",
      "|4.55|4.8016\n",
      "|4.57|1.6586\n",
      "|4.57|2.9102\n",
      "|4.59|1.1453\n",
      "|4.53|3.0177\n",
      "|4.65|4.4882\n",
      "|4.58|5.7909\n",
      "|4.59|6.0972\n",
      "|4.70|4.1165\n",
      "|4.57|5.5922\n",
      "|4.57|6.3628\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 2.41     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2375     |\n",
      "|    total timesteps  | 48939    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0558   |\n",
      "|    n_updates        | 2250     |\n",
      "----------------------------------\n",
      "|4.55|4.7846\n",
      "|4.63|6.0618\n",
      "|4.65|-3.9902\n",
      "|4.61|-2.0941\n",
      "|4.60|-9.4929\n",
      "|4.49|1.7992\n",
      "|4.67|3.0921\n",
      "|4.57|5.7517\n",
      "|4.58|1.5862\n",
      "|4.62|5.0656\n",
      "|2.33|2.3641\n",
      "|4.63|5.9047\n",
      "|4.53|6.2463\n",
      "|4.58|4.9770\n",
      "Eval num_timesteps=50000, episode_reward=5.71 +/- 0.54\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | 5.71     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0595   |\n",
      "|    n_updates        | 2310     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "|4.65|6.3669\n",
      "|4.66|6.0564\n",
      "|4.65|4.1340\n",
      "|4.61|5.2035\n",
      "done\n",
      "|4.13|41.0082\n",
      "done\n",
      "|4.04|41.2806\n",
      "|4.55|6.4208\n",
      "|4.58|5.0571\n",
      "|4.60|6.2039\n",
      "|4.56|8.2925\n",
      "|4.64|2.8030\n",
      "|4.54|-0.9327\n",
      "|4.60|2.1060\n",
      "|4.56|4.4929\n",
      "|4.67|4.9625\n",
      "|4.61|3.4634\n",
      "|4.63|5.4986\n",
      "|4.66|3.8972\n",
      "|4.67|4.6328\n",
      "|4.51|5.8704\n",
      "|4.63|4.5589\n",
      "|4.54|5.9335\n",
      "|4.67|-2.8855\n",
      "|4.62|7.3473\n",
      "done\n",
      "|4.66|40.4956\n",
      "done\n",
      "|4.33|40.5280\n",
      "|4.61|1.3851\n",
      "|4.60|7.9361\n",
      "|4.50|5.3000\n",
      "|4.54|4.1892\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 4.54     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2574     |\n",
      "|    total timesteps  | 53000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0621   |\n",
      "|    n_updates        | 2450     |\n",
      "----------------------------------\n",
      "|4.63|2.6633\n",
      "|4.55|6.5006\n",
      "|4.62|0.1121\n",
      "|4.67|6.4351\n",
      "|4.68|0.2658\n",
      "|4.54|8.2868\n",
      "|4.64|5.2471\n",
      "|4.48|0.6097\n",
      "done\n",
      "|3.99|42.1795\n",
      "|4.62|1.9215\n",
      "|4.56|-3.2730\n",
      "|4.53|0.6685\n",
      "|4.58|3.4212\n",
      "|4.57|0.8954\n",
      "|4.55|-1.4631\n",
      "|4.62|6.0956\n",
      "|4.61|8.6261\n",
      "done\n",
      "|4.18|41.2612\n",
      "|4.66|4.8628\n",
      "|4.62|0.4318\n",
      "|0.26|2.7344\n",
      "|4.61|6.9470\n",
      "|4.57|5.3669\n",
      "|4.55|4.6811\n",
      "Eval num_timesteps=55000, episode_reward=5.66 +/- 0.95\n",
      "Episode length: 101.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 101      |\n",
      "|    mean_reward      | 5.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0614   |\n",
      "|    n_updates        | 2560     |\n",
      "----------------------------------\n",
      "|4.58|9.8282\n",
      "done\n",
      "|4.30|40.5838\n",
      "|4.60|6.6395\n",
      "|4.54|-1.1543\n",
      "done\n",
      "|4.56|40.5524\n",
      "|4.58|4.3467\n",
      "|4.60|-4.5765\n",
      "|4.58|-3.0979\n",
      "done\n",
      "|4.10|41.0041\n",
      "|4.56|6.6562\n",
      "|4.56|4.5466\n",
      "|4.61|7.5467\n",
      "|4.55|-8.7186\n",
      "|4.51|-6.3876\n",
      "|4.64|-6.3996\n",
      "|4.55|-6.8871\n",
      "|4.62|5.2540\n",
      "|4.45|-9.4160\n",
      "|4.65|-9.4929\n",
      "|4.47|-9.4930\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 6.13     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2769     |\n",
      "|    total timesteps  | 57001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.068    |\n",
      "|    n_updates        | 2650     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "|3.98|42.6706\n",
      "|4.55|7.7908\n",
      "done\n",
      "|3.74|41.6343\n",
      "done\n",
      "|3.72|42.0236\n",
      "done\n",
      "|3.43|43.0779\n",
      "done\n",
      "|3.70|42.8663\n",
      "done\n",
      "|4.17|41.5268\n",
      "done\n",
      "|4.04|41.8921\n",
      "done\n",
      "|3.91|41.8309\n",
      "done\n",
      "|3.97|41.6845\n",
      "done\n",
      "|4.16|40.7630\n",
      "done\n",
      "|4.06|41.9301\n",
      "done\n",
      "|4.25|41.2990\n",
      "done\n",
      "|4.03|41.3233\n",
      "done\n",
      "|3.61|42.3994\n",
      "done\n",
      "|3.71|42.2545\n",
      "done\n",
      "|4.42|41.0928\n",
      "done\n",
      "|3.94|41.3975\n",
      "done\n",
      "|3.89|41.1052\n",
      "|4.56|8.9776\n",
      "done\n",
      "|4.42|40.2379\n",
      "done\n",
      "|4.18|41.3460\n",
      "done\n",
      "|3.59|42.2041\n",
      "done\n",
      "|3.61|41.9611\n",
      "done\n",
      "|3.56|42.2468\n",
      "done\n",
      "|3.88|42.8008\n",
      "done\n",
      "|3.85|42.2467\n",
      "done\n",
      "|4.43|40.8775\n",
      "done\n",
      "|3.81|41.6194\n",
      "done\n",
      "|3.65|42.0108\n",
      "done\n",
      "|3.82|42.5505\n",
      "done\n",
      "|3.87|41.9362\n",
      "done\n",
      "|3.97|41.4785\n",
      "done\n",
      "|3.90|42.0456\n",
      "|1.80|5.6242\n",
      "done\n",
      "|3.62|41.8569\n",
      "done\n",
      "|3.73|42.7382\n",
      "done\n",
      "|3.73|42.1974\n",
      "Eval num_timesteps=60000, episode_reward=42.26 +/- 0.36\n",
      "Episode length: 82.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 82       |\n",
      "|    mean_reward      | 42.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0668   |\n",
      "|    n_updates        | 2830     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "done\n",
      "|4.20|41.3764\n",
      "done\n",
      "|3.72|41.7651\n",
      "done\n",
      "|4.28|40.7626\n",
      "|4.55|-8.4405\n",
      "|4.52|-5.3465\n",
      "|4.55|-5.5999\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.8     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2942     |\n",
      "|    total timesteps  | 60569    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0808   |\n",
      "|    n_updates        | 2850     |\n",
      "----------------------------------\n",
      "done\n",
      "|3.16|44.4888\n",
      "done\n",
      "|3.93|41.3780\n",
      "done\n",
      "|2.63|45.1334\n",
      "done\n",
      "|3.32|43.5368\n",
      "done\n",
      "|2.08|45.4888\n",
      "done\n",
      "|2.95|44.8487\n",
      "done\n",
      "|2.54|45.0164\n",
      "done\n",
      "|2.27|45.0149\n",
      "done\n",
      "|3.67|43.6344\n",
      "|4.57|1.1200\n",
      "done\n",
      "|2.98|43.5544\n",
      "done\n",
      "|3.02|44.2458\n",
      "done\n",
      "|2.68|44.9611\n",
      "done\n",
      "|2.72|44.2975\n",
      "done\n",
      "|3.04|43.5735\n",
      "done\n",
      "|2.75|45.4978\n",
      "done\n",
      "|2.67|44.9175\n",
      "done\n",
      "|2.64|45.8099\n",
      "done\n",
      "|3.10|43.8110\n",
      "done\n",
      "|2.86|43.7503\n",
      "done\n",
      "|2.91|44.3593\n",
      "done\n",
      "|2.64|45.5106\n",
      "done\n",
      "|2.94|43.8136\n",
      "done\n",
      "|3.01|44.1761\n",
      "done\n",
      "|3.76|41.9915\n",
      "done\n",
      "|2.78|43.7515\n",
      "done\n",
      "|2.72|44.7101\n",
      "done\n",
      "|2.44|46.4625\n",
      "done\n",
      "|2.81|44.8229\n",
      "done\n",
      "|2.25|45.5930\n",
      "done\n",
      "|2.71|44.4560\n",
      "done\n",
      "|2.10|45.7945\n",
      "done\n",
      "|3.75|41.6474\n",
      "done\n",
      "|3.82|41.1900\n",
      "done\n",
      "|3.34|42.4549\n",
      "done\n",
      "|2.85|45.1811\n",
      "done\n",
      "|2.68|45.4869\n",
      "done\n",
      "|2.72|44.2278\n",
      "done\n",
      "|2.36|45.3456\n",
      "done\n",
      "|2.62|44.1238\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.4     |\n",
      "|    ep_rew_mean      | 33       |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 3059     |\n",
      "|    total timesteps  | 63136    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0829   |\n",
      "|    n_updates        | 3050     |\n",
      "----------------------------------\n",
      "done\n",
      "|3.27|43.5181\n",
      "done\n",
      "|2.99|44.6828\n",
      "done\n",
      "|2.75|44.1679\n",
      "done\n",
      "|2.78|45.5553\n",
      "done\n",
      "|2.55|45.7503\n",
      "|4.55|8.3563\n",
      "done\n",
      "|3.23|43.3274\n",
      "done\n",
      "|2.36|46.7398\n",
      "|4.60|-3.5424\n",
      "|4.56|-9.4930\n",
      "done\n",
      "|2.45|45.2918\n",
      "done\n",
      "|2.73|45.7308\n",
      "done\n",
      "|2.77|44.4943\n",
      "done\n",
      "|2.97|44.6561\n",
      "done\n",
      "|2.36|46.8976\n",
      "done\n",
      "|1.97|46.8693\n",
      "|4.56|-3.6090\n",
      "|4.50|2.1779\n",
      "done\n",
      "|2.39|45.3448\n",
      "done\n",
      "|4.54|40.4725\n",
      "done\n",
      "|2.83|44.7778\n",
      "done\n",
      "|2.84|44.4743\n",
      "done\n",
      "|2.34|46.5582\n",
      "done\n",
      "|2.59|45.5523\n",
      "done\n",
      "|2.26|45.5909\n",
      "done\n",
      "|2.16|46.5551\n",
      "done\n",
      "|2.30|45.5159\n",
      "done\n",
      "|2.34|45.4662\n",
      "|0.15|0.5615\n",
      "done\n",
      "|2.09|47.4008\n",
      "done\n",
      "|1.94|45.7142\n",
      "done\n",
      "|2.12|46.0109\n",
      "Eval num_timesteps=65000, episode_reward=46.38 +/- 0.74\n",
      "Episode length: 45.00 +/- 1.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 45       |\n",
      "|    mean_reward      | 46.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.01     |\n",
      "|    loss             | 0.0859   |\n",
      "|    n_updates        | 3200     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "done\n",
      "|2.07|46.7598\n",
      "done\n",
      "|2.02|47.4617\n",
      "Evaluating...\n",
      "|0.03|0.0000\n",
      "1 -> [0.  0.8] 0.66\n",
      "2 -> [0.2 0.8] -0.59\n",
      "1 -> [0.2  0.66] 0.48\n",
      "1 -> [0.2  0.52] 0.97\n",
      "1 -> [0.2  0.36] 1.48\n",
      "1 -> [0.2  0.22] 1.57\n",
      "1 -> [0.2  0.06] 2.03\n",
      "2 -> [0.4  0.06] -1.07\n",
      "2 -> [0.6  0.06] -0.57\n",
      "3 -> [0.6  0.26] -0.1\n",
      "3 -> [0.6  0.46] -0.1\n",
      "3 -> [0.6  0.66] -0.1\n",
      "3 -> [0.6  0.85] -0.1\n",
      "0 -> [0.4  0.85] -0.1\n",
      "0 -> [0.21 0.85] -0.1\n",
      "1 -> [0.21 0.66] 0.38\n",
      "1 -> [0.21 0.5 ] 1.13\n",
      "1 -> [0.21 0.35] 1.39\n",
      "1 -> [0.21 0.18] 1.84\n",
      "1 -> [0.21 0.04] 1.92\n",
      "2 -> [0.41 0.04] -1.07\n",
      "2 -> [0.61 0.04] -0.58\n",
      "3 -> [0.61 0.24] -0.1\n",
      "3 -> [0.61 0.44] -0.1\n",
      "3 -> [0.61 0.64] -0.1\n",
      "3 -> [0.61 0.84] -0.1\n",
      "0 -> [0.41 0.84] -0.1\n",
      "0 -> [0.21 0.84] -0.1\n",
      "1 -> [0.21 0.68] 0.28\n",
      "1 -> [0.21 0.55] 0.83\n",
      "1 -> [0.21 0.4 ] 1.22\n",
      "1 -> [0.21 0.26] 1.44\n",
      "1 -> [0.21 0.13] 1.58\n",
      "2 -> [0.41 0.13] -0.91\n",
      "2 -> [0.61 0.13] -0.39\n",
      "3 -> [0.61 0.33] -0.1\n",
      "3 -> [0.61 0.52] -0.1\n",
      "3 -> [0.61 0.72] -0.1\n",
      "3 -> [0.61 0.92] -0.1\n",
      "0 -> [0.42 0.92] -0.1\n",
      "0 -> [0.23 0.92] -0.1\n",
      "1 -> [0.23 0.72] -0.1\n",
      "1 -> [0.23 0.56] 0.89\n",
      "1 -> [0.23 0.43] 1.04\n",
      "done\n",
      "1 -> [0.23 0.28] 31.47\n",
      "Evaluated 45.51559138298036\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "results = {}\n",
    "for v in vals:\n",
    "    for i in range(2):\n",
    "        print(\"Training\",v,i)\n",
    "        res = trainDQN(65000,*v,i,env)\n",
    "        results[str(v)+str(i)]=res\n",
    "        f = open(\"backup_results_new_7_appended.obj\",\"wb\")\n",
    "        pickle.dump(res,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28061707",
   "metadata": {},
   "source": [
    "## try retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76eb3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.verbose=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e7c76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DQN.load(\"models/dqn_13_more_free\")\n",
    "# modelO = DQN.load(\"models/dqn_13_more_free\")\n",
    "model = DQN.load(\"models/DQN_LR0.010_H1_64_H2_64_BS_1000_GS_10_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92f8de0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.learning_rate=0.001\n",
    "model.learning_starts=10\n",
    "model.target_update_interval=1000\n",
    "model.exploration_initial_eps=0.5\n",
    "model.exploration_final_eps=0.01\n",
    "model.set_env(env)\n",
    "model.draw=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0996786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.exploration_schedule = lambda x :  (0.2 - 0.2*(1-(x-0.05))) if x<0.96 else 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8998ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.draw=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
